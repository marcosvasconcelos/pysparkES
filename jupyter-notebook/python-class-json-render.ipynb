{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://0f2d0da75ef2:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>appName</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=appName>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName('appName').setMaster('local[*]')\n",
    "sc = SparkContext(conf=conf)\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PYTHON CLASS SERIALIZING JSON\n",
    "import json\n",
    "from json import JSONEncoder\n",
    "\n",
    "class Pessoa():\n",
    "    def __init__(self, nome, sobre_nome, num_cpf, num_identidade, dth_nascimento, endereco_pessoa, veiculo):\n",
    "        self.nome = nome\n",
    "        self.sobre_nome = sobre_nome\n",
    "        self.num_cpf = num_cpf\n",
    "        self.num_identidade = num_identidade\n",
    "        self.dth_nascimento = dth_nascimento\n",
    "        self.endereco_pessoa = endereco_pessoa\n",
    "        self.veiculo = veiculo\n",
    "        \n",
    "class Endereco():\n",
    "    def __init__(self, num_cep, cidade, uf, endereco, num_endereco, complemento):\n",
    "        self.num_cep = num_cep\n",
    "        self.cidade = cidade\n",
    "        self.uf = uf\n",
    "        self.endereco = endereco\n",
    "        self.num_endereco = num_endereco\n",
    "        self.complemento = complemento\n",
    "    \n",
    "class Veiculo():\n",
    "    def __init__(self, marca, modelo, tipo, ano):\n",
    "        self.tipo = tipo\n",
    "        self.marca = marca\n",
    "        self.modelo = modelo\n",
    "        self.ano = ano\n",
    "\n",
    "class Produto():\n",
    "    def __init__(self, nome_produto, cod_produto, desc_produto):\n",
    "        self.nome_produto = nome_produto\n",
    "        self.cod_produto = cod_produto\n",
    "        self.desc_produto = desc_produto\n",
    "        \n",
    "class Contrato():\n",
    "    def __init__(self, duracao, valor, parcelado, valor_parcela, desconto, valor_desconto,\n",
    "                 valor_total,ini_vigencia,fin_vigencia, dth_compra,num_apolice, produto, pessoa):\n",
    "        self.duracao = duracao\n",
    "        self.valor = valor\n",
    "        self.parcelado = parcelado\n",
    "        self.valor_parcela = valor_parcela\n",
    "        self.desconto = desconto\n",
    "        self.valor_desconto = valor_desconto\n",
    "        self.valor_total = valor_total\n",
    "        self.ini_vigencia = ini_vigencia\n",
    "        self.fin_vigencia = fin_vigencia\n",
    "        self.dth_compra = dth_compra\n",
    "        self.num_apolice = num_apolice\n",
    "        self.produto = produto\n",
    "        self.pessoa = pessoa\n",
    "        \n",
    "#subclass JSONEncoder\n",
    "class SeguroEncoder(JSONEncoder):\n",
    "        def default(self, o):\n",
    "            return o.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import hashlib\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta, MO\n",
    "\n",
    "def str_time_prop(start, end, format, prop):\n",
    "    stime = time.mktime(time.strptime(start, format))\n",
    "    etime = time.mktime(time.strptime(end, format))\n",
    "    ptime = stime + prop * (etime - stime)\n",
    "    return time.strftime(format, time.localtime(ptime))\n",
    "\n",
    "### Format date para o elasticsearch entender o field gerado como date\n",
    "def random_date(start, end, prop):\n",
    "    return str_time_prop(start, end, '%Y/%m/%d %I:%M:%S', prop)\n",
    "\n",
    "### Gera número de apólice randômica\n",
    "def random_num_apolice():\n",
    "    return random.randint(200000000000000,399999999999999)\n",
    "\n",
    "### Gera hash através da data e código para criar a chave _id para o documento no indice\n",
    "def random_hash(timestamp, cod):\n",
    "    cod_h = timestamp + cod\n",
    "    return str(hashlib.md5(cod_h.encode()).hexdigest())\n",
    "\n",
    "### Gera numero de CPF randomicamente usando um range padrão\n",
    "def random_cpf():\n",
    "    return random.randint(20000000001,88782514100)\n",
    "\n",
    "### Gerador veiculo\n",
    "def random_veiculo():\n",
    "    marcas = ['VW','TOYOTA','GM', 'FORD', 'HONDA']\n",
    "    modelos = [['jetta', 't-cross', 'amarok'],['camry', 'rav4', 'hilux'],\n",
    "               ['prisma', 'cruze', 's10'],['fusion','ecosport','ranger'],['civic', 'hrv','ridgeline']]\n",
    "    tipos = ['passeio','SUV','pick-up']\n",
    "    \n",
    "    marca_selected = random.choice(marcas)\n",
    "    ind_marca = marcas.index(marca_selected)\n",
    "    modelo = random.choice(modelos[ind_marca])\n",
    "    ind_modelo = modelos[ind_marca].index(modelo)\n",
    "    tipo = tipos[ind_modelo]\n",
    "    veiculo = [marca_selected, modelo, tipo, random.randint(2018,2020)]\n",
    "    return veiculo\n",
    "\n",
    "### Gerador endereco\n",
    "def random_endereco():\n",
    "    uf_array = ['DF', 'SP', 'RJ', 'GO', 'RS', 'PR', 'SC', 'BA', 'TO' ]\n",
    "    uf_rand = random.choice(uf_array)\n",
    "    cep_rand = random.randint(10000000,99999999)\n",
    "    endreco = [uf_rand, cep_rand]\n",
    "    return endreco\n",
    "\n",
    "### Gerador Produto\n",
    "def random_produto():\n",
    "    produtos = [['1021','Seguro auto premium','Seguro super premium de auto'], ['10111','Seguro auto light','Seguro auto simples'],\n",
    "                        ['2020','Seguro auto executive', 'Seguro para executivos'], ['10222','Seguro auto total', 'Seguro total nacional']]\n",
    "    produto_arr = random.choice(produtos)\n",
    "    return produto_arr\n",
    "### mudar a data de inicio da vigencia para ter um resultado próximo aos dias atuais.\n",
    "def random_contrato():\n",
    "    duracao = 12\n",
    "    valor = random.randint(1200,3000)\n",
    "    parcelado = \"true\"\n",
    "    valor_parcela = round((valor / duracao), 2)\n",
    "    desconto = \"false\"\n",
    "    valor_desconto = 0.0\n",
    "    valor_total = round((valor_parcela * duracao), 2)\n",
    "    ini_vigencia = random_date(\"2019/12/1 1:30:00\", \"2020/7/31 1:30:00\", random.random())\n",
    "    date_time_obj = datetime.strptime(ini_vigencia, '%Y/%m/%d %H:%M:%S') + relativedelta(months =+12)\n",
    "    fin_vigencia = str(date_time_obj)\n",
    "    dth_compra = ini_vigencia\n",
    "    num_apolice = random_num_apolice()\n",
    "    return [duracao, valor, parcelado, valor_parcela, desconto, valor_desconto,\n",
    "                 valor_total,ini_vigencia,fin_vigencia, dth_compra,int(num_apolice)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import random\n",
    "import json\n",
    "\n",
    "def gerador_contratos(): \n",
    "    ### Geradores randomicos\n",
    "    veiculo_rand = random_veiculo()\n",
    "    dt_nascimento = random_date(\"1975/1/1 1:30:00\", \"2000/12/31 1:30:00\", random.random())\n",
    "    endereco_rand = random_endereco()\n",
    "    produto_rand = random_produto()\n",
    "    contrato_rand = random_contrato()\n",
    "    ### Pupular classes para renderizar o json\n",
    "    veiculo = Veiculo(veiculo_rand[0],veiculo_rand[1],veiculo_rand[2], veiculo_rand[3])\n",
    "    endereco_pessoa = Endereco(endereco_rand[1], 'xxxxx', endereco_rand[0], 'Rua x apsd', 1, 'ap 82' )\n",
    "    pessoa = Pessoa('MMM', 'SILVA', random_cpf(), 123213, dt_nascimento , endereco_pessoa, veiculo)\n",
    "    produto = Produto(produto_rand[1], produto_rand[0], produto_rand[2])\n",
    "    contrato = Contrato(contrato_rand[0], contrato_rand[1], contrato_rand[2], contrato_rand[3], contrato_rand[4], contrato_rand[5],\n",
    "                     contrato_rand[6],contrato_rand[7],contrato_rand[8], contrato_rand[9],contrato_rand[10], produto, pessoa)\n",
    "    ### Renderizar o json de contrato\n",
    "    ## contratoJSON = json.dumps(contrato, indent=4, cls=SeguroEncoder)\n",
    "    contratoJSON = json.dumps(contrato, cls=SeguroEncoder)\n",
    "    return contratoJSON\n",
    "\n",
    "### Função de formatar a lista para o gerar o rdd\n",
    "def format_data(x):\n",
    "    return(x['num_apolice'], json.dumps(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Gerador de contratos\n",
    "## Quantidade de contratos desejados\n",
    "qtd_contrato = 10000\n",
    "event_list = []\n",
    "while qtd_contrato > 0:\n",
    "    event_list.append(json.loads(gerador_contratos()))\n",
    "    qtd_contrato = qtd_contrato - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gera do RDD para armazenamento no elasticsearch\n",
    "## A \n",
    "rdd = sc.parallelize(map(lambda x: format_data(x),event_list))\n",
    "\n",
    "## Configurações para persistência no elasticsearch\n",
    "conf = {\n",
    "    \"es.nodes\" : 'https://172.17.0.2',\n",
    "    \"es.port\" : '9200',\n",
    "    \"es.net.ssl\" : 'true',\n",
    "    \"es.resource\" : 'eventos',\n",
    "    \"es.input.json\" : 'yes',\n",
    "    \"es.mapping.id\": 'num_apolice',\n",
    "    \"es.net.http.auth.user\" :'admin',\n",
    "    \"es.net.http.auth.pass\" : 'admin'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.saveAsNewAPIHadoopFile.\n: org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only'\n\tat org.elasticsearch.hadoop.rest.InitializationUtils.discoverClusterInfo(InitializationUtils.java:340)\n\tat org.elasticsearch.hadoop.mr.EsOutputFormat.init(EsOutputFormat.java:262)\n\tat org.elasticsearch.hadoop.mr.EsOutputFormat.checkOutputSpecs(EsOutputFormat.java:235)\n\tat org.apache.spark.internal.io.HadoopMapReduceWriteConfigUtil.assertConf(SparkHadoopWriter.scala:393)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsNewAPIHadoopDataset$1(PairRDDFunctions.scala:1077)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopDataset(PairRDDFunctions.scala:1075)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsNewAPIHadoopFile$2(PairRDDFunctions.scala:994)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:985)\n\tat org.apache.spark.api.python.PythonRDD$.saveAsNewAPIHadoopFile(PythonRDD.scala:633)\n\tat org.apache.spark.api.python.PythonRDD.saveAsNewAPIHadoopFile(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.elasticsearch.hadoop.rest.EsHadoopTransportException: javax.net.ssl.SSLHandshakeException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\n\tat org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:128)\n\tat org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:432)\n\tat org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:428)\n\tat org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:388)\n\tat org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:392)\n\tat org.elasticsearch.hadoop.rest.RestClient.get(RestClient.java:168)\n\tat org.elasticsearch.hadoop.rest.RestClient.mainInfo(RestClient.java:745)\n\tat org.elasticsearch.hadoop.rest.InitializationUtils.discoverClusterInfo(InitializationUtils.java:330)\n\t... 29 more\nCaused by: javax.net.ssl.SSLHandshakeException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\n\tat java.base/sun.security.ssl.Alert.createSSLException(Alert.java:131)\n\tat java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:326)\n\tat java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:269)\n\tat java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:264)\n\tat java.base/sun.security.ssl.CertificateMessage$T13CertificateConsumer.checkServerCerts(CertificateMessage.java:1339)\n\tat java.base/sun.security.ssl.CertificateMessage$T13CertificateConsumer.onConsumeCertificate(CertificateMessage.java:1214)\n\tat java.base/sun.security.ssl.CertificateMessage$T13CertificateConsumer.consume(CertificateMessage.java:1157)\n\tat java.base/sun.security.ssl.SSLHandshake.consume(SSLHandshake.java:392)\n\tat java.base/sun.security.ssl.HandshakeContext.dispatch(HandshakeContext.java:444)\n\tat java.base/sun.security.ssl.HandshakeContext.dispatch(HandshakeContext.java:422)\n\tat java.base/sun.security.ssl.TransportContext.dispatch(TransportContext.java:183)\n\tat java.base/sun.security.ssl.SSLTransport.decode(SSLTransport.java:171)\n\tat java.base/sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1403)\n\tat java.base/sun.security.ssl.SSLSocketImpl.readHandshakeRecord(SSLSocketImpl.java:1309)\n\tat java.base/sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:440)\n\tat java.base/sun.security.ssl.SSLSocketImpl.ensureNegotiated(SSLSocketImpl.java:814)\n\tat java.base/sun.security.ssl.SSLSocketImpl$AppOutputStream.write(SSLSocketImpl.java:1184)\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n\tat java.base/java.io.BufferedOutputStream.flush(BufferedOutputStream.java:142)\n\tat org.apache.commons.httpclient.HttpConnection.flushRequestOutputStream(HttpConnection.java:828)\n\tat org.apache.commons.httpclient.HttpMethodBase.writeRequest(HttpMethodBase.java:2116)\n\tat org.apache.commons.httpclient.HttpMethodBase.execute(HttpMethodBase.java:1096)\n\tat org.apache.commons.httpclient.HttpMethodDirector.executeWithRetry(HttpMethodDirector.java:398)\n\tat org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:171)\n\tat org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:397)\n\tat org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:323)\n\tat org.elasticsearch.hadoop.rest.commonshttp.CommonsHttpTransport.doExecute(CommonsHttpTransport.java:685)\n\tat org.elasticsearch.hadoop.rest.commonshttp.CommonsHttpTransport.execute(CommonsHttpTransport.java:664)\n\tat org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:116)\n\t... 36 more\nCaused by: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\n\tat java.base/sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:439)\n\tat java.base/sun.security.validator.PKIXValidator.engineValidate(PKIXValidator.java:306)\n\tat java.base/sun.security.validator.Validator.validate(Validator.java:264)\n\tat java.base/sun.security.ssl.X509TrustManagerImpl.validate(X509TrustManagerImpl.java:313)\n\tat java.base/sun.security.ssl.X509TrustManagerImpl.checkTrusted(X509TrustManagerImpl.java:222)\n\tat java.base/sun.security.ssl.X509TrustManagerImpl.checkServerTrusted(X509TrustManagerImpl.java:129)\n\tat java.base/sun.security.ssl.CertificateMessage$T13CertificateConsumer.checkServerCerts(CertificateMessage.java:1323)\n\t... 60 more\nCaused by: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\n\tat java.base/sun.security.provider.certpath.SunCertPathBuilder.build(SunCertPathBuilder.java:141)\n\tat java.base/sun.security.provider.certpath.SunCertPathBuilder.engineBuild(SunCertPathBuilder.java:126)\n\tat java.base/java.security.cert.CertPathBuilder.build(CertPathBuilder.java:297)\n\tat java.base/sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:434)\n\t... 66 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-100f942bc6d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## executa o método de persistencia no elasticsearch de acordo com as configurações para persistência.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m rdd.saveAsNewAPIHadoopFile(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0moutputFormatClass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"org.elasticsearch.hadoop.mr.EsOutputFormat\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mkeyClass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"org.apache.hadoop.io.NullWritable\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msaveAsNewAPIHadoopFile\u001b[0;34m(self, path, outputFormatClass, keyClass, valueClass, keyConverter, valueConverter, conf)\u001b[0m\n\u001b[1;32m   1519\u001b[0m         \u001b[0mjconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dictToJavaMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m         \u001b[0mpickledRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pickled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1521\u001b[0;31m         self.ctx._jvm.PythonRDD.saveAsNewAPIHadoopFile(pickledRDD._jrdd, True, path,\n\u001b[0m\u001b[1;32m   1522\u001b[0m                                                        \u001b[0moutputFormatClass\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1523\u001b[0m                                                        \u001b[0mkeyClass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalueClass\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.saveAsNewAPIHadoopFile.\n: org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only'\n\tat org.elasticsearch.hadoop.rest.InitializationUtils.discoverClusterInfo(InitializationUtils.java:340)\n\tat org.elasticsearch.hadoop.mr.EsOutputFormat.init(EsOutputFormat.java:262)\n\tat org.elasticsearch.hadoop.mr.EsOutputFormat.checkOutputSpecs(EsOutputFormat.java:235)\n\tat org.apache.spark.internal.io.HadoopMapReduceWriteConfigUtil.assertConf(SparkHadoopWriter.scala:393)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsNewAPIHadoopDataset$1(PairRDDFunctions.scala:1077)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopDataset(PairRDDFunctions.scala:1075)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsNewAPIHadoopFile$2(PairRDDFunctions.scala:994)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:985)\n\tat org.apache.spark.api.python.PythonRDD$.saveAsNewAPIHadoopFile(PythonRDD.scala:633)\n\tat org.apache.spark.api.python.PythonRDD.saveAsNewAPIHadoopFile(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.elasticsearch.hadoop.rest.EsHadoopTransportException: javax.net.ssl.SSLHandshakeException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\n\tat org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:128)\n\tat org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:432)\n\tat org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:428)\n\tat org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:388)\n\tat org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:392)\n\tat org.elasticsearch.hadoop.rest.RestClient.get(RestClient.java:168)\n\tat org.elasticsearch.hadoop.rest.RestClient.mainInfo(RestClient.java:745)\n\tat org.elasticsearch.hadoop.rest.InitializationUtils.discoverClusterInfo(InitializationUtils.java:330)\n\t... 29 more\nCaused by: javax.net.ssl.SSLHandshakeException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\n\tat java.base/sun.security.ssl.Alert.createSSLException(Alert.java:131)\n\tat java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:326)\n\tat java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:269)\n\tat java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:264)\n\tat java.base/sun.security.ssl.CertificateMessage$T13CertificateConsumer.checkServerCerts(CertificateMessage.java:1339)\n\tat java.base/sun.security.ssl.CertificateMessage$T13CertificateConsumer.onConsumeCertificate(CertificateMessage.java:1214)\n\tat java.base/sun.security.ssl.CertificateMessage$T13CertificateConsumer.consume(CertificateMessage.java:1157)\n\tat java.base/sun.security.ssl.SSLHandshake.consume(SSLHandshake.java:392)\n\tat java.base/sun.security.ssl.HandshakeContext.dispatch(HandshakeContext.java:444)\n\tat java.base/sun.security.ssl.HandshakeContext.dispatch(HandshakeContext.java:422)\n\tat java.base/sun.security.ssl.TransportContext.dispatch(TransportContext.java:183)\n\tat java.base/sun.security.ssl.SSLTransport.decode(SSLTransport.java:171)\n\tat java.base/sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1403)\n\tat java.base/sun.security.ssl.SSLSocketImpl.readHandshakeRecord(SSLSocketImpl.java:1309)\n\tat java.base/sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:440)\n\tat java.base/sun.security.ssl.SSLSocketImpl.ensureNegotiated(SSLSocketImpl.java:814)\n\tat java.base/sun.security.ssl.SSLSocketImpl$AppOutputStream.write(SSLSocketImpl.java:1184)\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n\tat java.base/java.io.BufferedOutputStream.flush(BufferedOutputStream.java:142)\n\tat org.apache.commons.httpclient.HttpConnection.flushRequestOutputStream(HttpConnection.java:828)\n\tat org.apache.commons.httpclient.HttpMethodBase.writeRequest(HttpMethodBase.java:2116)\n\tat org.apache.commons.httpclient.HttpMethodBase.execute(HttpMethodBase.java:1096)\n\tat org.apache.commons.httpclient.HttpMethodDirector.executeWithRetry(HttpMethodDirector.java:398)\n\tat org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:171)\n\tat org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:397)\n\tat org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:323)\n\tat org.elasticsearch.hadoop.rest.commonshttp.CommonsHttpTransport.doExecute(CommonsHttpTransport.java:685)\n\tat org.elasticsearch.hadoop.rest.commonshttp.CommonsHttpTransport.execute(CommonsHttpTransport.java:664)\n\tat org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:116)\n\t... 36 more\nCaused by: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\n\tat java.base/sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:439)\n\tat java.base/sun.security.validator.PKIXValidator.engineValidate(PKIXValidator.java:306)\n\tat java.base/sun.security.validator.Validator.validate(Validator.java:264)\n\tat java.base/sun.security.ssl.X509TrustManagerImpl.validate(X509TrustManagerImpl.java:313)\n\tat java.base/sun.security.ssl.X509TrustManagerImpl.checkTrusted(X509TrustManagerImpl.java:222)\n\tat java.base/sun.security.ssl.X509TrustManagerImpl.checkServerTrusted(X509TrustManagerImpl.java:129)\n\tat java.base/sun.security.ssl.CertificateMessage$T13CertificateConsumer.checkServerCerts(CertificateMessage.java:1323)\n\t... 60 more\nCaused by: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\n\tat java.base/sun.security.provider.certpath.SunCertPathBuilder.build(SunCertPathBuilder.java:141)\n\tat java.base/sun.security.provider.certpath.SunCertPathBuilder.engineBuild(SunCertPathBuilder.java:126)\n\tat java.base/java.security.cert.CertPathBuilder.build(CertPathBuilder.java:297)\n\tat java.base/sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:434)\n\t... 66 more\n"
     ]
    }
   ],
   "source": [
    "## executa o método de persistencia no elasticsearch de acordo com as configurações para persistência.\n",
    "rdd.saveAsNewAPIHadoopFile(\n",
    "    path='-',\n",
    "    outputFormatClass=\"org.elasticsearch.hadoop.mr.EsOutputFormat\",\n",
    "    keyClass=\"org.apache.hadoop.io.NullWritable\",\n",
    "    valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\",\n",
    "    conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}